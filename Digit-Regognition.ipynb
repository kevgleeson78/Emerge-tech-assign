{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital recognition with the mnist dataset\n",
    "\n",
    "This notebook will investigate the classification and identification of hand written digits using a neural network.<br/>\n",
    "The mnist dataset will be first used to train the network and then test the networks performance in recognising a digit.<br/>\n",
    "Once training has been completed a single image from the dataset will be passed to the network and the result will be displayed to the screen along with the actual digit expected.<br/>\n",
    "![Mnist Image](https://corochann.com/wp-content/uploads/2017/02/mnist_plot-800x600.png)\n",
    "<cite>Image source https://corochann.com/wp-content/uploads/2017/02/mnist_plot-800x600.png</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages needed for the program to run\n",
    "\n",
    "The following packages will need to be imported for creating the network and importing the images to memory:\n",
    "* The keras package used for creating the network\n",
    "* The gzip package used for unzipping the the dataset images and labels\n",
    "* The numpy package used for altering the dataset into numpy arrays\n",
    "* The sklearn preprocessing package used for classification and binary encoding each digit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the packages \n",
    "import keras as kr\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as pre\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "To begin we need to initialise the netowrk using the sequential model.<br/>\n",
    "This allows us to add layers as we need them. <br/>\n",
    "These layers can be tweaked to increse performance.<br/>\n",
    "We will investigate this later in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the neural network\n",
    "model = kr.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the layers to the network\n",
    "To add layers to the network the layers method from keras will be used.<br/>\n",
    "There will be a dense connection between neurons meaning that every neuron from the input is connected to every neuron in the middle layer and every neuron frim the middle layer is connected to every neuron on the output layer.\n",
    "\n",
    "![Neural Network](https://cdn-images-1.medium.com/max/800/1*jYhgQ4I_oFdxgDD-AOgV1w.png)\n",
    "<cite>Image source https://cdn-images-1.medium.com/max/800/1*jYhgQ4I_oFdxgDD-AOgV1w.pngS</cite>\n",
    "\n",
    "* In the below code segment the units attribute represents the amount of neurons that will be in the middle layer in this case we have 1000 neurons.<br/>\n",
    "* The activation attribute sets the activation function in this case we are using  [relu activation](https://keras.io/activations/) the relu activation has a steeper gradient than softmax and as a result speeds up the training process wothout the loss of performance. \n",
    "\n",
    "* The final attribute is used to set the amount of input neurons the network has. In the below exapmle the number is set to 784 as this is equal to the number of bytes each image has within the mnist dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a hidden(middle layer) with 1000 neurons and an input layer with 784.\n",
    "# There are 784 input neurons as this value is equal to the total amount of bytes each image has.\n",
    "model.add(kr.layers.Dense(units=1000, activation='relu', input_dim=784))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output layer\n",
    "The output layer has ten neurons that will map to the amount of training labels that are within the dataset. The predicted results are sent from the middle layer to the ouput layer and compared to the actual number that has benn sent in as image data.<br/>\n",
    "The closer to the value one the result is the more accurate the algorithm is perfoming.<br/>\n",
    "This process is repeated and the point of gradent decent converges towards the base of the slope. <br/>\n",
    "The process ends when all of the epochs have completed which will be explained later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ten neurons to the output layer\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "The compile method is used to build the model based on each layer created along with their connections specified in the above cell.</br>\n",
    "* The first argument [categorical_crossentropy](https://keras.io/losses/) creates a vector to hold the values of each digit as a binary representation, this will be set with the pre.LabelBinarizer() to be discussed further in this notebook.\n",
    "* The second optimizer argument is set to [stochastic gradient descent optimizer](https://keras.io/optimizers/) This sets the learning rate, and the decay of this learning rate over time.\n",
    "* The final argument [metrics](https://keras.io/metrics/) is used to output the performance to the neural network after each run of data has been sent from the central layer to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening the files in .gz format\n",
    "\n",
    "As dicussed in my previous [mnist notebook](https://github.com/kevgleeson78/Emerge-tech-assign/blob/master/Mnist%20Dataset.ipynb) the gzipped files are opened and read using the gzip package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the gzipped files and read as bytes.\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data into memory\n",
    "\n",
    "Each of the 60000 images and labels are then stored inot their respective variables.<br/>\n",
    "We divde by 255 to convert the grey scale value to a value between one and zero.<br/>\n",
    "These values are then used by the neural netwrok in conjunction with the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all images and labels into memory\n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "train_lbl =  np.array(list(train_lbl[8:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening the data into a single array\n",
    "The data is converter from a three dimensional array to a one dimensional array where all of the image bytes (28 *28) 784 are sequentailly stored one after another.<br/>\n",
    "This techneque is used so each byte representing the image can be have a one to one mappimg to the neural networks input layer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Flattening the data into a single array\n",
    "The data is converter from a three dimensional array to a single one dimensional array where all of the image bytes (28 *28) 784 are sequentailly stored one after another.<br/>\n",
    "This is used so each byte representing the image can be have a one to one mappimg to the neural networks input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98823529, 0.92941176, 0.92941176,\n",
       "        0.92941176, 0.50588235, 0.46666667, 0.31372549, 0.89803922,\n",
       "        0.34901961, 0.        , 0.03137255, 0.50196078, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.88235294, 0.85882353, 0.63137255, 0.39607843,\n",
       "        0.33333333, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.11764706, 0.3254902 , 0.00784314, 0.05098039,\n",
       "        0.23529412, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.80784314, 0.06666667,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.01568627, 0.63529412,\n",
       "        0.67843137, 0.67843137, 0.78039216, 0.84705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92941176, 0.14117647, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.28627451,\n",
       "        0.03137255, 0.05490196, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68627451, 0.38823529, 0.58039216, 0.00784314, 0.00784314,\n",
       "        0.19607843, 0.95686275, 1.        , 0.83137255, 0.39607843,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.94509804,\n",
       "        0.99607843, 0.39607843, 0.00784314, 0.64705882, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.45490196,\n",
       "        0.00784314, 0.25490196, 0.99215686, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.95686275, 0.25490196, 0.00784314,\n",
       "        0.7254902 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.8627451 , 0.05490196, 0.11764706, 0.37254902,\n",
       "        0.57647059, 0.99607843, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.68235294, 0.05882353, 0.00784314, 0.00784314, 0.53333333,\n",
       "        0.90196078, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.82352941,\n",
       "        0.27058824, 0.00784314, 0.00784314, 0.41176471, 0.89411765,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9372549 , 0.63529412,\n",
       "        0.01176471, 0.00784314, 0.26666667, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.02352941, 0.00784314,\n",
       "        0.02352941, 0.74901961, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.81960784, 0.49019608,\n",
       "        0.28235294, 0.00784314, 0.00784314, 0.18823529, 0.99215686,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84705882,\n",
       "        0.41960784, 0.10196078, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.01960784, 0.28627451, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.90588235, 0.55294118, 0.13333333, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.21176471, 0.69411765, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.90980392, 0.74117647, 0.16470588, 0.00784314,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.22352941, 0.68235294,\n",
       "        0.99215686, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.92941176, 0.32941176, 0.14117647,\n",
       "        0.00784314, 0.00784314, 0.00784314, 0.00784314, 0.23529412,\n",
       "        0.68627451, 0.96470588, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.78431373, 0.3254902 ,\n",
       "        0.11372549, 0.00784314, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.04313725, 0.47843137, 0.95686275, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.46666667, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.16862745, 0.47058824, 0.48235294, 0.9372549 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the array so the inputs can be mapped to the input neurons\n",
    "inputs = train_img.reshape(60000, 784)\n",
    "inputs[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the data\n",
    "The label data is encoded into a matrix of 10 x 10 this will represent the didgits in binary format.\n",
    "Firstly we to setup the matrix using the labelBinerizer function.<br/>\n",
    "The fit function passes the training labels as a argument. AS the set of labels are from zero - nine the (encoder.fit) function generates a matrix based on these values. In this case it will be a 10 x 10 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the labels into binary format\n",
    "encoder = pre.LabelBinarizer()\n",
    "# get the size of the array needed for each category\n",
    "encoder.fit(train_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the labels\n",
    "The labels are then transformed to a binary value based on the deimal value of the label.</br>\n",
    "With each number being transfromed to the following:\n",
    "* (0) 1000000000\n",
    "* (1) 0100000000\n",
    "* (3) 0010000000 \n",
    "\n",
    "And so on until we reach the number nine which is 0000000001.<br/>\n",
    "\n",
    "\n",
    "In the below example the number five has the representation of '0 0 0 0 0 1 0 0 0 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# encode each label to be used as binary outputs\n",
    "outputs = encoder.transform(train_lbl)\n",
    "# print out the integer value and the new representation of the number\n",
    "print(train_lbl[0], outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full example\n",
    "Below is a full view of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[1 0 0 0 0 0 0 0 0 0]]\n",
      "1 [[0 1 0 0 0 0 0 0 0 0]]\n",
      "2 [[0 0 1 0 0 0 0 0 0 0]]\n",
      "3 [[0 0 0 1 0 0 0 0 0 0]]\n",
      "4 [[0 0 0 0 1 0 0 0 0 0]]\n",
      "5 [[0 0 0 0 0 1 0 0 0 0]]\n",
      "6 [[0 0 0 0 0 0 1 0 0 0]]\n",
      "7 [[0 0 0 0 0 0 0 1 0 0]]\n",
      "8 [[0 0 0 0 0 0 0 0 1 0]]\n",
      "9 [[0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# print out each array\n",
    "for i in range(10):\n",
    "    print(i, encoder.transform([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trianing the model\n",
    "We are now ready to begin training the netwrok to regognise the images.</br>\n",
    "The training set of 60000 images are used and passed to the networks first layer of 784 neurons.<br/>\n",
    "Model parameters:\n",
    "1. The encoded training images are sent as input\n",
    "2. The encided trainig labels are attached as the expected output\n",
    "3. epochs is the amount of times the 60000 images will be processed \n",
    "4. The batch size sets the amount of images that will be sent to the network as one unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 17s 280us/step - loss: 0.8760 - acc: 0.7824\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.4585 - acc: 0.8772\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.3910 - acc: 0.8896\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.3599 - acc: 0.8980\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.3392 - acc: 0.9029\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.3248 - acc: 0.9067\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.3134 - acc: 0.91040s - loss: 0.3130 - acc: 0.910\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.3029 - acc: 0.9131\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.2950 - acc: 0.9163\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.2861 - acc: 0.9187\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.2790 - acc: 0.9202\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 256us/step - loss: 0.2722 - acc: 0.9222\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.2651 - acc: 0.92520s - loss: 0.2660 -\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.2589 - acc: 0.9261\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.2524 - acc: 0.92841s \n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 16s 259us/step - loss: 0.2463 - acc: 0.9310\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 16s 260us/step - loss: 0.2399 - acc: 0.9328\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.2347 - acc: 0.9348\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 16s 263us/step - loss: 0.2286 - acc: 0.9361\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 16s 273us/step - loss: 0.2234 - acc: 0.9380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a47f204278>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training\n",
    "# Set the model up by adding the input and output layers to the network\n",
    "#The epochs value is the amount of test runs are needed\n",
    "# The batch_size value is the amount of images sent at one time to the network\n",
    "model.fit(inputs, outputs, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
